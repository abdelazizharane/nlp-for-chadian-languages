{
  "hash": "dbe6a2737757aa58db1b267bb1f915a2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle : \"Collecte des données sur les langues tchadiennes\"\nauthor: \"Abdel-aziz Harane\"\ndate: \"2025-02-12\"\ncategories: [Data, Article]\n---\n\n\nCollecte et traitement des données pour *SHU* avec `R` et `Python`\n\nDans un projet NLP, la phase la plus complexe est la collecte et le traitement des données. Pour ce projet, c'est un défi majeur (qui m'a pris 4 mois), notamment en raison du manque de ressources structurées et de la diversité des formats de données. Dans cet article, nous allons aborder **les différentes étapes et défis liés à l’extraction, la transformation et la structuration des données linguistiques** obtenues à partir de sources variées (textes religieux, vocabulaires, grammaires) en utilisant **R** et **Python**.\n\nJ'espère qu'il vous sera utile. Il y a beaucoup de choses à discuter ici du côté pratique de cette collecte, mais j'essaierai d'être plus précis en vous fournissant que les informations nécessaires.\n\n#### Les points abordés dans cet article :\n\n-   `Scraping avec le package rvest de R`: Extraction de textes de la Bible en arabe tchadien (shu)\n-   `Extraction de textes depuis des PDF`: Pourquoi l’OCR pose problème sur certains documents anciens et mal structurés\n-   `Nettoyage et prétraitement des données`: Techniques pour normaliser et améliorer la qualité des textes extraits\n-   `Structuration et tokenization des données`: Transformation des données pour une meilleure exploitation en NLP\n-   `Export et intégration des données`: Formats adaptés pour entraîner des modèles NLP\n\n### 1. Scraping avec R et `rvest`\n\nUne grande partie des textes disponibles en langues tchadiennes sont dispersés sur des sites non structurés. Le package **rvest** permet d’extraire ces textes efficacement.\n\n\n```{text}\nlibrary(\"rvest\")\nurl <- \"https://bible.com/arabe-tchadien\"\npage <- read_html(url)\ntextes <- page %>% html_nodes(\"p\") %>% html_text()\nwriteLines(textes, \"bible_arabe_tchadien.txt\") \n```\n\n\n#### Défis rencontrés :\n\n-   Certains sites affichent du texte en JavaScript, ce qui rend l’extraction plus complexe.\n-   Les chapitres et versets sont parfois imbriqués dans des balises non standardisées.\n\n**Solutions possibles :**\n\n-   Utilisation de Selenium pour exécuter le JavaScript avant d’extraire les données.\n-   Extraction et nettoyage des balises avec des expressions régulières.\n\n### 2. Extraction de textes depuis des PDF\n\nLes PDF contenant des documents en langues tchadiennes sont souvent **anciens et mal structurés**. Même les outils OCR comme `tesseract` rencontrent des difficultés.\n\n#### Problèmes observés :\n\n-   Mauvaise segmentation des caractères (ex : \"hi-fatad yeen\" devient \"hifatad yeen\").\n-   Perte d’information due à des polices de caractères non standardisées\n-   Disposition des colonnes non uniforme, ce qui fausse l’extraction\n\n\n```{text}\n#install.packages(\"reticulate\")\nretic\nimport pytesseract\nfrom pdf2image import convert_from_path\n\npages = convert_from_path(\"dico_shu.pdf\")\n\nfor i, page in enumerate(pages):\n    text = pytesseract.image_to_string(page, lang='ara')\n    with open(f\"output_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(text)\n```\n\n\n#### Solutions et améliorations :\n\n-   Utiliser **pdfminer.six** pour extraire directement le texte au lieu de l'OCR si la couche texte est présente.\n-   **Segmenter les colonnes manuellement** pour reconstruire des phrases cohérentes.\n-   Convertir les documents en **format HTML** avant de les traiter pour un alignement plus précis.\n\n### 3. Nettoyage et prétraitement des données\n\nUne fois les données extraites, elles nécessitent un prétraitement rigoureux avant d’être utilisées pour le NLP.\n\n#### Étapes principales :\n\n-   **Normalisation des caractères** (suppression des accents, homogénéisation de l’écriture des mots).\n-   **Correction des erreurs OCR** avec des dictionnaires de mots.\n-   **Segmentation des phrases et tokenization**.\n\n\n```{text}\nimport re  #bibliothèque des expressions régulières\n\ndef clean_text(text):\n    \"\"\"\n    Fonction de prétraitement du texte :\n    - Convertit le texte en minuscules\n    - Supprime les caractères spéciaux (hors lettres, chiffres et espaces)\n    \"\"\"\n    text = text.lower()\n    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Suppression des caractères spéciaux\n    return text\n\n# exemple d'utilisation\ntext_cleaned = clean_text(\"Allâh est mis en forme étrangement !\")\n\nprint(text_cleaned)  \n# Sortie : 'allh est mis en forme etrangement'\n```\n\n\n**Prétraitements standards :**\n\n-   Passage en minuscules\n-   Suppression de la ponctuation, des nombres et des stop words et la racinisation (stemming)\n\n### 4. Structuration et tokenization des données\n\nPour rendre les données exploitables par un modèle NLP, nous devons structurer les textes et segmenter les mots de manière efficace.\n\n-   Passage en minusculeske\n-   Suppression de la ponctuation, des nombres et des stop words et la racinisation (stemming)\n\n\n```{text}\nCerertains mots en ltexte <- \"Allah est grand et miséricordieux\"\ntokens <- tokenize_words(texte)\nprint(tokens)\n```\n\n\nCertains mots en langues tchadiennes sont **concaténés** et difficiles à segmenter sans un lexique adapté.\n\n-   Certaines langues utilisent **des déclinaisons riches**, rendant la tokenization plus complexe.\n\n**Vous devez envisager à:**\n\n-   Développer un tokenizer spécifique en utilisant un **modèle statistique basé sur les fréquences des mots**.\n-   Entraîner un **modèle de segmentation de mots** en deep learning pour affiner la tokenization.\n\n",
    "supporting": [
      "data_collecte_files"
    ],
    "filters": [],
    "includes": {}
  }
}