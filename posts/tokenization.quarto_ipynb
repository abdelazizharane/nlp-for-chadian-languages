{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Tokenization des langues tchadiennes\"\n",
        "author: \"Abdel-aziz Harane\"\n",
        "date: \"2024-02-20\"\n",
        "categories: [Tokenization, Preprocessing]\n",
        "---\n",
        "\n",
        "\n",
        "Dans ce guide, je partage avec vous les approches que nous avons adopt√©es pour concevoir un tokenizer adapt√© √† `l‚Äôarabe tchadien (Shu)`, une m√©thodologie √©galement applicable √† d‚Äôautres langues locales. Nous explorerons ensemble la litt√©rature existante, les principaux algorithmes de tokenization et je vous expliquerai en d√©tail le processus d‚Äôimpl√©mentation de notre propre mod√®le.\n",
        "\n",
        "::: callout-note\n",
        "La cr√©ation d‚Äôun `tokenizer` est une √©tape essentielle lorsqu‚Äôon souhaite entra√Æner un mod√®le √† partir de z√©ro ou affiner (*fine-tuning*) un mod√®le pr√©-entra√Æn√©.\n",
        ":::\n",
        "\n",
        "En effet, les tokenizers g√©n√©riques ne prennent pas en compte les sp√©cificit√©s linguistiques et morphologiques de nos langues, ce qui rend n√©cessaire le d√©veloppement d'un nouveau tokenizer. Sans cette adaptation, les mod√®les existants peinent √† capturer les sp√©cificit√©s linguistiques et morphologiques des langues locales.\n",
        "\n",
        "Dans le cadre de ce projet, **nous avons appliqu√© trois algorithmes de tokenization distincts**, chacun pr√©sentant ses avantages et ses limites. Cette d√©marche nous a permis d‚Äôexplorer en profondeur le fonctionnement de la tokenization et d‚Äôanalyser son impact sur l‚Äôarabe tchadien (*Shu*).\n",
        "\n",
        "Nous d√©butons par une pr√©sentation de la tokenization : sa d√©finition, son importance et son mode de fonctionnement. Ensuite, nous passerons en revue les trois m√©thodes que nous avons exp√©riment√©es, afin d‚Äôen comprendre les sp√©cificit√©s et d‚Äô√©valuer leur pertinence pour notre projet.\n",
        "\n",
        "*Sans plus tarder, commen√ßons. On file !*\n",
        "\n",
        "> **Qu'est-ce qu'un tokenizer ?**\n",
        "\n",
        "Pour qu‚Äôun ordinateur puisse lire et comprendre une phrase, il est d‚Äôabord n√©cessaire de la transformer en une repr√©sentation num√©rique, un processus appel√© *vectorisation* (rendre les mots en vecteur num√©rique). En effet, une machine ne comprend pas les mots sous leur forme brute. Pour elle, une phrase n‚Äôest qu‚Äôune succession de caract√®res sans signification intrins√®que. Or, les ordinateurs ne traitent que des suites de 0 et de 1.\n",
        "\n",
        "C‚Äôest ici qu‚Äôintervient la ***tokenization*** : un m√©canisme qui permet de d√©couper une phrase en unit√©s plus petites ‚Äì appel√©es ***tokens*** ‚Äì afin de les rendre compr√©hensibles pour un mod√®le d‚Äôintelligence artificielle (IA). Une fois ce d√©coupage effectu√©, chaque token est ensuite converti en une valeur num√©rique, une √©tape connue sous le nom d‚Äô***encodage***.\n",
        "\n",
        "Ainsi, une phrase est repr√©sent√©e sous forme de vecteurs, comme par exemple : **`[129, 103, 192]`** .\n",
        "\n",
        "Par exemple, la phrase :\n",
        "\n",
        "\n",
        "```{text}\n",
        "\"Zahra indaha khalag asfar\" Peut √™tre transform√©e en :\n",
        "\n",
        "\"Zahra\"\n",
        "\"indaha\"\n",
        "\"khalag\"\n",
        "\"asfar\"\n",
        "```\n",
        "\n",
        "\n",
        "Ces morceaux sont appel√©s des **tokens**, et l'ordinateur peut maintenant travailler avec eux plus facilement.\n",
        "\n",
        "### **Les 3 algorithmes les plus utilis√©s üëáüèø**\n",
        "\n",
        "::: panel-tabset\n",
        "## Byte Pair Encoding (BPE)\n",
        "\n",
        "> ***C'est quoi BPE ?***\n",
        "\n",
        "> Lors qu'on veut apprendre √† √©crire un texte en utilisant le moins de place possible ou plut√¥t que de stocker chaque mot individuellement, nous pouvons donc rep√©rer les lettres ou les groupes de lettres les plus fr√©quents et les remplacer par un symbole plus court.\n",
        "\n",
        "\n",
        "```{text}\n",
        "- Phrase originale : Zahra indaha khalag asfar\n",
        "- D√©coupage en tokens : [\"Za\", \"hra\", \"inda\", \"ha\", \"kha\", \"lag\", \"as\", \"far\"]\n",
        "- Apr√®s BPE : [\"#A\", \"inda\", \"ha\", \"#B\"] (o√π #A et #B sont des unit√©s apprises)\n",
        "```\n",
        "\n",
        "\n",
        "## Sentencepiece\n",
        "\n",
        "> ***C'est quoi Sentencepiece ?***\n",
        "\n",
        "Imagineons que nous avons une phrase et voulons la d√©couper, mais nous ne voulons pas seulement couper aux espaces (car certaines langues n'ont pas d'espaces clairs entre les mots comme le Chinois). *Sentencepiece* apprend √† d√©couper la phrase en morceaux de mani√®re plus flexible.\n",
        "\n",
        "\n",
        "```{text}\n",
        "- Phrase originale : Al-naadum da gaa'id fil-beet\n",
        "- Tokenization avec SentencePiece : [Al-\", \"naadum\", \"da\", \"gaa'id\", \"fil-\", \"beet]\n",
        "- SentencePiece peut aussi d√©couper : [\"Al\", \"na\", \"adum\", \"da\", \"gaa'\", \"id\", \"fil\", \"beet\"] pour plus de flexibilit√©.\n",
        "```\n",
        "\n",
        "\n",
        "## WordPiece\n",
        "\n",
        "> ***C'est quoi WordPiece ?***\n",
        "\n",
        "WordPiece est une m√©thode de tokenization qui segmente une phrase en sous-unit√©s en privil√©giant les s√©quences les plus fr√©quemment rencontr√©es dans les donn√©es d'entra√Ænement. Elle est notamment utilis√©e dans des mod√®les comme *BERT* et GPT.\n",
        "\n",
        "\n",
        "```{text}\n",
        "- Phrase originale : \"Zahra indaha khalag asfar\"\n",
        "- Tokenization avec WordPiece : **[\"Zah\", \"##ra\", \"inda\", \"##ha\", \"khal\", \"##ag\", \"as\", \"##far\"]**\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "> Les langues tchadiennes et la tokenization\n",
        "\n",
        "Les langues tchadiennes, comme le Toupouri, Mundang et le Sara, pr√©sentent des d√©fis en tokenization. L'arabe tchadien, par exemple, utilise une √©criture attach√©e et des mots qui peuvent √™tre tr√®s longs lorsqu'ils contiennent des pr√©fixes et suffixes. Il est donc essentiel de tester plusieurs algorithmes et si besoin d'en cr√©er un autre (ce que nous avons fait pour l'arabe tchadien et Sara).\n",
        "\n",
        "Cela permet de garder des unit√©s fr√©quentes tout en fragmentant les mots moins courants.\n",
        "\n",
        "Ces trois m√©thodes nous ont permit de mieux adapter la tokenization en cr√©ant notre propre `tokenizer` sur le vocabulaire de `shu`. Cette approche pourrais √™tre √©tendue √† d'autres langues locales telles que : Sara, Kanembou, Mundang, Zaghawa, ... C'est une premi√®re exp√©rimentation et nous l'adapterons √† nos autres langues locales.\n",
        "\n",
        "Pour garantir une meilleure prise en charge, il est essentiel de tester plusieurs algorithmes et, si n√©cessaire, d‚Äôen d√©velopper un nouveau. Cette premi√®re exp√©rimentation sera progressivement affin√©e et adapt√©e √† d‚Äôautres langues tchadiennes.\n",
        "\n",
        "Si ce guide vous a √©t√© utile, n‚Äôh√©sitez pas √† le partager avec vos pairs !"
      ],
      "id": "d90f781e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\HP\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}