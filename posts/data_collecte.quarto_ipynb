{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title : \"Collecte des données sur les langues tchadiennes\"\n",
        "author: \"Abdel-aziz Harane\"\n",
        "date: \"2025-02-12\"\n",
        "categories: [Data, Article]\n",
        "---\n",
        "\n",
        "\n",
        "Collecte et traitement des données pour *SHU* avec `R` et `Python`\n",
        "\n",
        "Dans un projet NLP, la phase la plus complexe est la collecte et le traitement des données. Pour ce projet, c'est un défi majeur (qui m'a pris 4 mois), notamment en raison du manque de ressources structurées et de la diversité des formats de données. Dans cet article, nous allons aborder **les différentes étapes et défis liés à l’extraction, la transformation et la structuration des données linguistiques** obtenues à partir de sources variées (textes religieux, vocabulaires, grammaires) en utilisant **R** et **Python**.\n",
        "\n",
        "J'espère qu'il vous sera utile. Il y a beaucoup de choses à discuter ici du côté pratique de cette collecte, mais j'essaierai d'être plus précis en vous fournissant que les informations nécessaires.\n",
        "\n",
        "#### Les points abordés dans cet article :\n",
        "\n",
        "-   `Scraping avec le package rvest de R`: Extraction de textes de la Bible en arabe tchadien (shu)\n",
        "-   `Extraction de textes depuis des PDF`: Pourquoi l’OCR pose problème sur certains documents anciens et mal structurés\n",
        "-   `Nettoyage et prétraitement des données`: Techniques pour normaliser et améliorer la qualité des textes extraits\n",
        "-   `Structuration et tokenization des données`: Transformation des données pour une meilleure exploitation en NLP\n",
        "-   `Export et intégration des données`: Formats adaptés pour entraîner des modèles NLP\n",
        "\n",
        "### 1. Scraping avec R et `rvest`\n",
        "\n",
        "Une grande partie des textes disponibles en langues tchadiennes sont dispersés sur des sites non structurés. Le package **rvest** permet d’extraire ces textes efficacement.\n",
        "\n",
        "\n",
        "```{text}\n",
        "library(\"rvest\")\n",
        "url <- \"https://bible.com/arabe-tchadien\"\n",
        "page <- read_html(url)\n",
        "textes <- page %>% html_nodes(\"p\") %>% html_text()\n",
        "writeLines(textes, \"bible_arabe_tchadien.txt\") \n",
        "```\n",
        "\n",
        "\n",
        "#### Défis rencontrés :\n",
        "\n",
        "-   Certains sites affichent du texte en JavaScript, ce qui rend l’extraction plus complexe.\n",
        "-   Les chapitres et versets sont parfois imbriqués dans des balises non standardisées.\n",
        "\n",
        "**Solutions possibles :**\n",
        "\n",
        "-   Utilisation de Selenium pour exécuter le JavaScript avant d’extraire les données.\n",
        "-   Extraction et nettoyage des balises avec des expressions régulières.\n",
        "\n",
        "### 2. Extraction de textes depuis des PDF\n",
        "\n",
        "Les PDF contenant des documents en langues tchadiennes sont souvent **anciens et mal structurés**. Même les outils OCR comme `tesseract` rencontrent des difficultés.\n",
        "\n",
        "#### Problèmes observés :\n",
        "\n",
        "-   Mauvaise segmentation des caractères (ex : \"hi-fatad yeen\" devient \"hifatad yeen\").\n",
        "-   Perte d’information due à des polices de caractères non standardisées\n",
        "-   Disposition des colonnes non uniforme, ce qui fausse l’extraction\n",
        "\n",
        "\n",
        "```{text}\n",
        "#install.packages(\"reticulate\")\n",
        "retic\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "pages = convert_from_path(\"dico_shu.pdf\")\n",
        "\n",
        "for i, page in enumerate(pages):\n",
        "    text = pytesseract.image_to_string(page, lang='ara')\n",
        "    with open(f\"output_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "```\n",
        "\n",
        "\n",
        "#### Solutions et améliorations :\n",
        "\n",
        "-   Utiliser **pdfminer.six** pour extraire directement le texte au lieu de l'OCR si la couche texte est présente.\n",
        "-   **Segmenter les colonnes manuellement** pour reconstruire des phrases cohérentes.\n",
        "-   Convertir les documents en **format HTML** avant de les traiter pour un alignement plus précis.\n",
        "\n",
        "### 3. Nettoyage et prétraitement des données\n",
        "\n",
        "Une fois les données extraites, elles nécessitent un prétraitement rigoureux avant d’être utilisées pour le NLP.\n",
        "\n",
        "#### Étapes principales :\n",
        "\n",
        "-   **Normalisation des caractères** (suppression des accents, homogénéisation de l’écriture des mots).\n",
        "-   **Correction des erreurs OCR** avec des dictionnaires de mots.\n",
        "-   **Segmentation des phrases et tokenization**.\n",
        "\n",
        "\n",
        "```{text}\n",
        "import re  #bibliothèque des expressions régulières\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Fonction de prétraitement du texte :\n",
        "    - Convertit le texte en minuscules\n",
        "    - Supprime les caractères spéciaux (hors lettres, chiffres et espaces)\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Suppression des caractères spéciaux\n",
        "    return text\n",
        "\n",
        "# exemple d'utilisation\n",
        "text_cleaned = clean_text(\"Allâh est mis en forme étrangement !\")\n",
        "\n",
        "print(text_cleaned)  \n",
        "# Sortie : 'allh est mis en forme etrangement'\n",
        "```\n",
        "\n",
        "\n",
        "**Prétraitements standards :**\n",
        "\n",
        "-   Passage en minuscules\n",
        "-   Suppression de la ponctuation, des nombres et des stop words et la racinisation (stemming)\n",
        "\n",
        "### 4. Structuration et tokenization des données\n",
        "\n",
        "Pour rendre les données exploitables par un modèle NLP, nous devons structurer les textes et segmenter les mots de manière efficace.\n",
        "\n",
        "-   Passage en minusculeske\n",
        "-   Suppression de la ponctuation, des nombres et des stop words et la racinisation (stemming)\n",
        "\n",
        "\n",
        "```{text}\n",
        "Cerertains mots en ltexte <- \"Allah est grand et miséricordieux\"\n",
        "tokens <- tokenize_words(texte)\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "\n",
        "Certains mots en langues tchadiennes sont **concaténés** et difficiles à segmenter sans un lexique adapté.\n",
        "\n",
        "-   Certaines langues utilisent **des déclinaisons riches**, rendant la tokenization plus complexe.\n",
        "\n",
        "**Vous devez envisager à:**\n",
        "\n",
        "-   Développer un tokenizer spécifique en utilisant un **modèle statistique basé sur les fréquences des mots**.\n",
        "-   Entraîner un **modèle de segmentation de mots** en deep learning pour affiner la tokenization."
      ],
      "id": "df6a8395"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\HP\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}