---
title: "Tokenization des langues tchadiennes"
author: "Abdel-aziz Harane"
date: "2024-02-20"
categories: [Tokenization, Preprocessing]
---

Dans ce guide, je partage avec vous les approches que nous avons adoptÃ©es pour concevoir un tokenizer adaptÃ© Ã  `lâ€™arabe tchadien (Shu)`, une mÃ©thodologie Ã©galement applicable Ã  dâ€™autres langues locales. Nous explorerons ensemble la littÃ©rature existante, les principaux algorithmes de tokenization et je vous expliquerai en dÃ©tail le processus dâ€™implÃ©mentation de notre propre modÃ¨le.

::: callout-note
La crÃ©ation dâ€™un `tokenizer` est une Ã©tape essentielle lorsquâ€™on souhaite entraÃ®ner un modÃ¨le Ã  partir de zÃ©ro ou affiner (*fine-tuning*) un modÃ¨le prÃ©-entraÃ®nÃ©.
:::

En effet, les tokenizers gÃ©nÃ©riques ne prennent pas en compte les spÃ©cificitÃ©s linguistiques et morphologiques de nos langues, ce qui rend nÃ©cessaire le dÃ©veloppement d'un nouveau tokenizer. Sans cette adaptation, les modÃ¨les existants peinent Ã  capturer les spÃ©cificitÃ©s linguistiques et morphologiques des langues locales.

Dans le cadre de ce projet, **nous avons appliquÃ© trois algorithmes de tokenization distincts**, chacun prÃ©sentant ses avantages et ses limites. Cette dÃ©marche nous a permis dâ€™explorer en profondeur le fonctionnement de la tokenization et dâ€™analyser son impact sur lâ€™arabe tchadien (*Shu*).

Nous dÃ©butons par une prÃ©sentation de la tokenization : sa dÃ©finition, son importance et son mode de fonctionnement. Ensuite, nous passerons en revue les trois mÃ©thodes que nous avons expÃ©rimentÃ©es, afin dâ€™en comprendre les spÃ©cificitÃ©s et dâ€™Ã©valuer leur pertinence pour notre projet.

*Sans plus tarder, commenÃ§ons. On file !*

> **Qu'est-ce qu'un tokenizer ?**

Pour quâ€™un ordinateur puisse lire et comprendre une phrase, il est dâ€™abord nÃ©cessaire de la transformer en une reprÃ©sentation numÃ©rique, un processus appelÃ© *vectorisation* (rendre les mots en vecteur numÃ©rique). En effet, une machine ne comprend pas les mots sous leur forme brute. Pour elle, une phrase nâ€™est quâ€™une succession de caractÃ¨res sans signification intrinsÃ¨que. Or, les ordinateurs ne traitent que des suites de 0 et de 1.

Câ€™est ici quâ€™intervient la ***tokenization*** : un mÃ©canisme qui permet de dÃ©couper une phrase en unitÃ©s plus petites â€“ appelÃ©es ***tokens*** â€“ afin de les rendre comprÃ©hensibles pour un modÃ¨le dâ€™intelligence artificielle (IA). Une fois ce dÃ©coupage effectuÃ©, chaque token est ensuite converti en une valeur numÃ©rique, une Ã©tape connue sous le nom dâ€™***encodage***.

Ainsi, une phrase est reprÃ©sentÃ©e sous forme de vecteurs, comme par exemple : **`[129, 103, 192]`** .

Par exemple, la phrase :

```{text}
"Zahra indaha khalag asfar" Peut Ãªtre transformÃ©e en :

"Zahra"
"indaha"
"khalag"
"asfar"
```

Ces morceaux sont appelÃ©s des **tokens**, et l'ordinateur peut maintenant travailler avec eux plus facilement.

### **Les 3 algorithmes les plus utilisÃ©s ğŸ‘‡ğŸ¿**

::: panel-tabset
## Byte Pair Encoding (BPE)

> ***C'est quoi BPE ?***

> Lors qu'on veut apprendre Ã  Ã©crire un texte en utilisant le moins de place possible ou plutÃ´t que de stocker chaque mot individuellement, nous pouvons donc repÃ©rer les lettres ou les groupes de lettres les plus frÃ©quents et les remplacer par un symbole plus court.

```{text}
- Phrase originale : Zahra indaha khalag asfar
- DÃ©coupage en tokens : ["Za", "hra", "inda", "ha", "kha", "lag", "as", "far"]
- AprÃ¨s BPE : ["#A", "inda", "ha", "#B"] (oÃ¹ #A et #B sont des unitÃ©s apprises)
```

## Sentencepiece

> ***C'est quoi Sentencepiece ?***

Imagineons que nous avons une phrase et voulons la dÃ©couper, mais nous ne voulons pas seulement couper aux espaces (car certaines langues n'ont pas d'espaces clairs entre les mots comme le Chinois). *Sentencepiece* apprend Ã  dÃ©couper la phrase en morceaux de maniÃ¨re plus flexible.

```{text}
- Phrase originale : Al-naadum da gaa'id fil-beet
- Tokenization avec SentencePiece : [Al-", "naadum", "da", "gaa'id", "fil-", "beet]
- SentencePiece peut aussi dÃ©couper : ["Al", "na", "adum", "da", "gaa'", "id", "fil", "beet"] pour plus de flexibilitÃ©.
```

## WordPiece

> ***C'est quoi WordPiece ?***

WordPiece est une mÃ©thode de tokenization qui segmente une phrase en sous-unitÃ©s en privilÃ©giant les sÃ©quences les plus frÃ©quemment rencontrÃ©es dans les donnÃ©es d'entraÃ®nement. Elle est notamment utilisÃ©e dans des modÃ¨les comme *BERT* et GPT.

```{text}
- Phrase originale : "Zahra indaha khalag asfar"
- Tokenization avec WordPiece : **["Zah", "##ra", "inda", "##ha", "khal", "##ag", "as", "##far"]**
```
:::

> Les langues tchadiennes et la tokenization

Les langues tchadiennes, comme le Toupouri, Mundang et le Sara, prÃ©sentent des dÃ©fis en tokenization. L'arabe tchadien, par exemple, utilise une Ã©criture attachÃ©e et des mots qui peuvent Ãªtre trÃ¨s longs lorsqu'ils contiennent des prÃ©fixes et suffixes. Il est donc essentiel de tester plusieurs algorithmes et si besoin d'en crÃ©er un autre (ce que nous avons fait pour l'arabe tchadien et Sara).

Cela permet de garder des unitÃ©s frÃ©quentes tout en fragmentant les mots moins courants.

Ces trois mÃ©thodes nous ont permit de mieux adapter la tokenization en crÃ©ant notre propre `tokenizer` sur le vocabulaire de `shu`. Cette approche pourrais Ãªtre Ã©tendue Ã  d'autres langues locales telles que : Sara, Kanembou, Mundang, Zaghawa, ... C'est une premiÃ¨re expÃ©rimentation et nous l'adapterons Ã  nos autres langues locales.

Pour garantir une meilleure prise en charge, il est essentiel de tester plusieurs algorithmes et, si nÃ©cessaire, dâ€™en dÃ©velopper un nouveau. Cette premiÃ¨re expÃ©rimentation sera progressivement affinÃ©e et adaptÃ©e Ã  dâ€™autres langues tchadiennes.

Si ce guide vous a Ã©tÃ© utile, nâ€™hÃ©sitez pas Ã  le partager avec vos pairs !